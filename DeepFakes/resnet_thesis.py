# -*- coding: utf-8 -*-
"""ResNet_thesis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h2YaA5tlZHR313DRo17jb8SyrlkmAqNp
"""

#! git clone https://github.com/Sidd0511/Convolutional-Neural_Network.git

from tensorflow.keras.layers import Input, Lambda, Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.applications.resnet_v2 import ResNet152V2
from tensorflow.keras.applications.resnet_v2 import preprocess_input

IMAGE_SIZE = (224,224,3)

resnet_152v2 = ResNet152V2(include_top=False, weights=None, input_shape=IMAGE_SIZE)

for layer in resnet_152v2.layers[:-100]:
 layer.trainable = False

for layer in resnet_152v2.layers[-100:]:
 layer.trainable = True
#add average pooling and dense layer
pool_layer = GlobalAveragePooling2D(name = 'avg_pool')(resnet_152v2.output)

dense_1 = Dense(units=128, activation='softmax', name='fc1')(pool_layer)
dropout_1 = Dropout(rate=0.2)(dense_1)

dense_2 = Dense(units=64, activation='softmax', name='fc2')(dropout_1)
dropout_2 = Dropout(rate=0.2)(dense_2)

dense_3 = Dense(units=32, activation='softmax', name='fc3')(dropout_2)
dropout_3 = Dropout(rate=0.2)(dense_3)

predictions = Dense(units=2, activation='softmax', name='output')(dropout_2)

model = Model(inputs = resnet_152v2.input, outputs= predictions)

# model.summary()

model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('Training Set/',
                                                 target_size = (224, 224),
                                                 batch_size = 64,
                                                 class_mode = 'categorical')

test_set = test_datagen.flow_from_directory('Test Set/',
                                            target_size = (224, 224),
                                            batch_size = 64,
                                            class_mode = 'categorical',
                                            shuffle=False)

r = model.fit(
  training_set,
  validation_data=test_set,
  epochs=10,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set),
  verbose=2
)

# loss
plt.plot(r.history['loss'], label='train loss')
plt.plot(r.history['val_loss'], label='val loss')
plt.legend()
plt.show()
plt.savefig('LossVal_loss_deepfakes')

# accuracy
plt.plot(r.history['accuracy'], label='train acc')
plt.plot(r.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()
plt.savefig('AccVal_acc_deepfakes')

from sklearn.metrics import confusion_matrix, classification_report
import itertools

# function taken from "https://sites.google.com/view/amarnath-r/keras-deep-learning-image-classification-a-simple-example"
def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.figure(figsize=(10,10))

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        cm = np.around(cm, decimals=2)
        cm[np.isnan(cm)] = 0.0
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

Y_pred = model.predict(test_set, len(test_set))
y_pred = np.argmax(Y_pred, axis=1)
target_names = list(test_set.class_indices.keys())
print('Confusion Matrix')
conf_mat = confusion_matrix(test_set.classes, y_pred)
print('\n',conf_mat)
plot_confusion_matrix(conf_mat,target_names, title='Confusion Matrix')
print('Classification Report')
# target_names = ['Cats', 'Dogs', 'Horse']
print(classification_report(test_set.classes, y_pred, target_names=target_names))
